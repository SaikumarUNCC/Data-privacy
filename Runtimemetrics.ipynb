{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_nR9GfAYtp_"
      },
      "outputs": [],
      "source": [
        "pip install tensorflow numpy matplotlib opencv-python\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Licensed under the Creative Commons Attribution-NonCommercial 4.0 International License.\n",
        "\n",
        "\"\"\"Main module for evaluating GAN-based anonymization techniques using various metrics.\"\"\"\n",
        "\n",
        "import dnnlib\n",
        "from dnnlib import EasyDict\n",
        "import dnnlib.tflib as tflib\n",
        "import config\n",
        "from metrics import metric_base\n",
        "from training import misc\n",
        "\n",
        "def evaluate_metric(submit_config, metric_args, network_pkl, dataset_args, mirror_augment):\n",
        "    \"\"\"Evaluate a given metric on a specified network pickle file for anonymization performance.\"\"\"\n",
        "    ctx = dnnlib.RunContext(submit_config)\n",
        "    tflib.init_tf()\n",
        "    print(f'Evaluating {metric_args.name} metric on anonymization network \"{network_pkl}\"...')\n",
        "    metric = dnnlib.util.call_func_by_name(**metric_args)\n",
        "    metric.run(network_pkl, dataset_args=dataset_args, mirror_augment=mirror_augment, num_gpus=submit_config.num_gpus)\n",
        "    ctx.close()\n",
        "\n",
        "def evaluate_snapshot(submit_config, metric_args, run_id, snapshot):\n",
        "    \"\"\"Evaluate a metric on a specific training snapshot of an anonymization model.\"\"\"\n",
        "    ctx = dnnlib.RunContext(submit_config)\n",
        "    tflib.init_tf()\n",
        "    print(f'Evaluating {metric_args.name} metric on run_id {run_id}, snapshot {snapshot}...')\n",
        "    run_dir = misc.locate_run_dir(run_id)\n",
        "    network_pkl = misc.locate_network_pkl(run_dir, snapshot)\n",
        "    metric.run(network_pkl, run_dir=run_dir, num_gpus=submit_config.num_gpus)\n",
        "    ctx.close()\n",
        "\n",
        "def evaluate_all_snapshots(submit_config, metric_args, run_id):\n",
        "    \"\"\"Evaluate a metric across all snapshots of a specified run for an anonymization project.\"\"\"\n",
        "    ctx = dnnlib.RunContext(submit_config)\n",
        "    tflib.init_tf()\n",
        "    print(f'Evaluating {metric_args.name} metric on all snapshots of run_id {run_id} for anonymization performance...')\n",
        "    run_dir = misc.locate_run_dir(run_id)\n",
        "    network_pkls = misc.list_network_pkls(run_dir)\n",
        "    for idx, network_pkl in enumerate(network_pkls):\n",
        "        ctx.update('', idx, len(network_pkls))\n",
        "        metric.run(network_pkl, run_dir=run_dir, num_gpus=submit_config.num_gpus)\n",
        "    ctx.close()\n",
        "\n",
        "def main():\n",
        "    \"\"\"Configure and submit tasks for evaluating metrics on GAN-based anonymization techniques.\"\"\"\n",
        "    submit_config = dnnlib.SubmitConfig()\n",
        "    metrics = [metric_base.fid50k]  # Define which metrics to evaluate for anonymization effectiveness\n",
        "\n",
        "    # Define the tasks for metric evaluation\n",
        "    tasks = [\n",
        "        EasyDict(\n",
        "            run_func_name='run_metrics.evaluate_metric',\n",
        "            network_pkl='https://drive.google.com/uc?id=1MEGjdvVpUsu1jB4zrXZN7Y4kBBOzizDQ',\n",
        "            dataset_args=EasyDict(tfrecord_dir='ffhq', shuffle_mb=0),\n",
        "            mirror_augment=True\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    submit_config.num_gpus = 1  # Define number of GPUs to use\n",
        "    submit_config.run_dir_root = dnnlib.submission.submit.get_template_from_path(config.result_dir)\n",
        "    submit_config.run_dir_ignore += config.run_dir_ignore\n",
        "\n",
        "    for task in tasks:\n",
        "        for metric in metrics:\n",
        "            submit_config.run_desc = f'{task.run_func_name}-{metric.name}-{\"%dgpu\" % submit_config.num_gpus}'\n",
        "            dnnlib.submit_run(submit_config, metric_args=metric, **task)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "N1oAgTSPY8vu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}